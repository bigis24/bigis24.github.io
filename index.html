<!DOCTYPE html>
<!-- saved from url=(0026)https://bigis24.github.io/ -->
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Cayman is a clean, responsive theme for GitHub Pages." "="">
    <meta property=" og:image" content="https://kdd-milets.github.io/milets2022/assets/img/thumbnail.jpg">
    <meta property="og:type" content="website">
    <meta property="og:title" content="BigIS Workshop">
    <meta property="og:description" content="BigIS Workshop">


    <title>Information Seeking with Big Models Workshop</title>

    <!-- Bootstrap Core CSS -->
    <link href="./BigIS Workshop_files/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="./BigIS Workshop_files/agency.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="./BigIS Workshop_files/font-awesome.min.css" rel="stylesheet"
        type="text/css">
    <link href="./BigIS Workshop_files/css" rel="stylesheet" type="text/css">
    <link href="./BigIS Workshop_files/css(1)" rel="stylesheet" type="text/css">
    <link href="./BigIS Workshop_files/css(2)" rel="stylesheet" type="text/css">
    <link href="./BigIS Workshop_files/css(3)" rel="stylesheet" type="text/css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript" async=""
        src="./BigIS Workshop_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./BigIS Workshop_files/js"></script>
    <script type="text/javascript" async=""
        src="./BigIS Workshop_files/analytics(1).js"></script>
    <script type="text/javascript" async="" src="./BigIS Workshop_files/js(1)"></script>
    <script async="" src="./BigIS Workshop_files/js(2)"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-114495477-1');
    </script>


    <style type="text/css">
        .rpac-maincontainer-box .rpac-bgmask[data-v-f0f2b29a] {
            position: absolute;
            top: 22px;
            bottom: 0;
            right: 0;
            left: 0;
            background: rgba(0, 0, 0, 0);
        }

        .rpac-maincontainer-box .rpac-dragbarbox[data-v-f0f2b29a] {
            width: 100%;
            height: 24px;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .rpac-maincontainer-box .rpac-drag-bg[data-v-f0f2b29a] {
            display: inline-block;
            border-radius: 8px;
            cursor: pointer;
        }

        .rpac-maincontainer-box .rpac-drag-bg[data-v-f0f2b29a]:hover {
            background: rgba(25, 25, 25, 0.1);
        }

        .rpac-maincontainer-box .rpac-dragbar[data-v-f0f2b29a] {
            width: 32px;
            height: 6px;
            background: rgba(25, 25, 25, 0.2);
            border-radius: 4px;
            cursor: pointer;
            margin: 5px;
        }
    </style>
    <style type="text/css">
        .prompt-container[data-v-6d4faf98] {
            position: relative;
        }

        .prompt-container .modal-body[data-v-6d4faf98] {
            width: 600px;
            min-height: 285px;
            border: 1px solid #ffffff;
            border-radius: 16px;
            box-shadow: 0px 10px 50px -5px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(20px);
            background: rgba(255, 255, 255, 0.9);
            padding: 8px;
            display: flex;
            flex-direction: column;
        }

        .prompt-container .modal-body .modal-title[data-v-6d4faf98] {
            color: #191919;
            font-family: 鸿蒙黑体;
            font-size: 16px;
            font-weight: 700;
            line-height: 24px;
            letter-spacing: 0px;
            margin: 8px;
            margin-bottom: 12px;
        }

        .prompt-container .modal-body .modal-container[data-v-6d4faf98] {
            flex: 1;
            padding: 16px;
            border-radius: 12px;
            /* Shadow/Dark/10%-效果 */
            box-shadow: 0px 5px 50px -10px rgba(0, 0, 0, 0.1);
            background: #ffffff;
        }

        .prompt-container .modal-body .modal-container .title[data-v-6d4faf98] {
            color: #191919;
            font-family: 鸿蒙黑体;
            font-size: 16px;
            font-weight: 700;
            line-height: 24px;
            letter-spacing: 0px;
            margin-bottom: 12px;
        }

        .prompt-container .modal-body .modal-container .item[data-v-6d4faf98] {
            color: #191919;
            font-family: HarmonyOS Sans;
            font-size: 14px;
            font-weight: 400;
            line-height: 22px;
            letter-spacing: 0px;
            text-align: left;
            margin-bottom: 4px;
        }

        .prompt-container .modal-body .modal-container .footer[data-v-6d4faf98] {
            margin-top: 16px;
        }

        .prompt-container .modal-body .modal-container .footer .confirm-btn[data-v-6d4faf98] {
            margin-left: auto;
            width: fit-content;
            padding: 0 16px;
            height: 32px;
            line-height: 32px;
            color: #ffffff;
            font-family: 鸿蒙黑体;
            font-size: 14px;
            font-weight: 400;
            letter-spacing: 0px;
            border-radius: 20px;
            background: #1672f6;
        }

        .prompt-container .modal-body .modal-container .footer .confirm-btn[data-v-6d4faf98]:hover {
            cursor: pointer;
        }

        .prompt-container .modal-body .modal-container .footer .disabled[data-v-6d4faf98] {
            background: #cccccc;
            pointer-events: none;
        }

        .prompt-container .prompt-body[data-v-6d4faf98] {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            z-index: 9999;
            background: rgba(25, 25, 25, 0.5);
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .prompt-container .overtimes-prompt[data-v-6d4faf98] {
            position: fixed;
            right: 40px;
            bottom: 40px;
            z-index: 9999;
        }

        .prompt-container .overtimes-prompt .modal-body[data-v-6d4faf98] {
            width: 420px;
            min-height: 230px;
        }

        .prompt-container .overtimes-prompt .modal-body .gradient[data-v-6d4faf98] {
            background: linear-gradient(151.98deg, #ff914c, #ff4c4c);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            text-fill-color: transparent;
            font-family: 鸿蒙黑体;
            font-size: 16px;
            font-weight: 500;
            line-height: 12px;
            letter-spacing: 0px;
            text-align: left;
        }

        .prompt-container .osa-promt[data-v-6d4faf98] {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            background: rgba(25, 25, 25, 0.5);
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 9999;
        }

        .prompt-container .osa-promt .modal-body[data-v-6d4faf98] {
            width: 800px;
            min-height: 350px;
        }

        .prompt-container .osa-promt .modal-body .policy-page[data-v-6d4faf98] {
            color: #1672f6;
            font-family: HarmonyOS Sans;
            font-size: 14px;
            line-height: 32px;
            font-weight: 400;
            letter-spacing: 0px;
            text-align: left;
        }

        .prompt-container .osa-promt .modal-body .policy-page[data-v-6d4faf98]:hover {
            cursor: pointer;
        }

        .prompt-container .osa-promt .modal-body .footer[data-v-6d4faf98] {
            display: flex;
        }
    </style>
</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top navbar-shrink">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="https://bigis24.github.io/#page-top"></a>
                    </li>
                    <!--                 <li>
                    <a class="page-scroll" href="#summary">Intro</a>
                </li>
                <li>
                    <a class="page-scroll" href="#contributions">contributions</a>
                </li>
                <li>
                    <a class="page-scroll" href="#speaker">Speakers</a>
                </li>
                <li>
                    <a class="page-scroll" href="#call">Call for Papers</a>
                </li>
                <li>
                    <a class="page-scroll" href="#dates">Important Dates</a>
                </li>
                <li>
                    <a class="page-scroll" href="#schedule">Schedule</a>
                </li>
                <li>
                    <a class="page-scroll" href="#organizer">Organizer</a>
                </li>
                <li>
                    <a class="page-scroll" href="#contact">Contact</a>
                </li> -->
                    <li class="active">
                        <a class="page-scroll" href="https://bigis24.github.io/#summary">Intro</a>
                    </li>
                    <!--<li>
                        <a class="page-scroll" href="https://bigis24.github.io/#schedule">Schedule</a>
                    </li>-->
                    <li>
                        <a class="page-scroll" href="https://bigis24.github.io/#call">Call for Papers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="https://bigis24.github.io/#organizer">Organizer</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="https://bigis24.github.io/#speakers">Speaker</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="https://bigis24.github.io/#schedule">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="https://bigis24.github.io/#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- <div class="container">
        <div class="intro-text" style="color:white;">

            <div class="intro-heading">Workshop on Information Seeking with Big Models</div>
            <div class="intro-lead-in"> on the International Conference on Data Mining 2024 (ICDM'24)
            </div>
            <div class="intro-lead-in"> Abu Dhabi, UAE, 9-12 December 2024
            </div>
        </div>
    </div> -->

    <div style="padding-top: 70px;" class="bg-mid-gray">
        <!-- <div class="intro-heading" style="color:white;">-->
        <img src="./BigIS Workshop_files/bg.jpg" style="max-width:100%;height:100%;">
        <!-- <span style="position: absolute; top: 0; left: 0;">Information Seeking with Big Models Workshop</span> -->
        <!-- </div>-->
    </div>

    <section id="summary" style="padding-top: 80px;" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Scope and Topics</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                        This workshop is dedicated to pioneering research at the intersection of information seeking
                        (e.g., information retrieval, recommendation, and question-answering),
                        with the transformative potential of big models (e.g., language, visual, audio, and multimodal
                        models), specifically focusing on five key aspects.
                        (1) It encourages exploring how big models can revolutionize information retrieval strategies,
                        emphasizing advancements that enhance efficiency and effectiveness in accessing information.
                        (2) We encourage active researchers to utilize big models to advance recommender algorithms,
                        enhancing user modeling for improved personalized recommendations.
                        (3) We encourage groundbreaking research in question-answering systems, leveraging the
                        capabilities of big models to provide accurate and contextually relevant responses.
                        (4) We emphasize the critical role of trustworthiness when employing big models for information
                        seeking, to ensure the reliability of generated content with ethical and legal standards.
                        (5) We encourage researchers to design robust evaluation methodologies, standards, and human
                        evaluation paradigms to comprehensively assess the impact of big models in information seeking.
                    </p>

                    <p class="large text-muted">
                        In summary, the workshop will focus on exploring the challenges and opportunities of integrating
                        big models for information seeking, covering areas such as information retrieval,
                        recommendation, and question-answering. Our aim is to offer a platform for participants to
                        present their research, share experiences, and foster collaborative discussions.
                    </p>

                </div>
            </div>
        </div>
    </section>


    <!-- <section id="schedule" class="">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Schedule</h2>
                    h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>
                </div>
            </div>
            <p class="large text-muted" align="center">
                TBA
            </p>
            <h3><b>Keynote 1</b></h3>

            <img src="https://cde.nus.edu.sg/ece/wp-content/uploads/sites/3/2021/05/zhengshou_stanford_photo.jpg"
                class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5">
            </a>

            <a href="https://sites.google.com/view/showlab">
                <font color=#5672f9><u>Prof. Mike Zheng Shou</u></font>
            </a> is a tenure-track Assistant Professor at National University of Singapore and a former Research
            Scientist at Facebook AI in the Bay Area. He holds a PhD degree from Columbia University in the City of New
            York, where he worked with Prof. Shih-Fu Chang. He was awarded the Wei Family Private Foundation Fellowship.
            He received the best paper finalist at CVPR'22 and the best student paper nomination at CVPR'17. His team
            won 1st place in multiple international challenges including ActivityNet 2017, EPIC-Kitchens 2022, Ego4D
            2022 & 2023. He is a Fellow of the National Research Foundation (NRF) Singapore and has been named on the
            Forbes 30 Under 30 Asia list.

            </p>

            <b>Talk Title:</b> Large Generative Models Meet Multimodal Video Intelligence
            </br>
            <b>Abstract:</b> In this talk, I'd like to share my recent research around multimodal video intelligence in
            the era of large generative models. I will first talk about video-language pretraining techniques
            (All-in-one, EgoVLP) that use one single model to power various understanding tasks ranging from retrieval
            to QA. Then I will introduce challenges and our efforts of adapting these large pretrained models to AI
            Assistant, such a real-world application (AssistQ, AssistGPT). Finally I will delve into the reverse problem
            i.e. given open-world textual description, how to generate videos (Tune-A-Video, Show-1).

            <br></br>

            <h3><b>Keynote 2</b></h3>


            <img src="https://lgm3a2023.github.io/LGM3A2023/LGM3A%20Workshop%20_%20ACM%20MM%202023_files/face.jpg"
                class="media-left pull-left" width="155" alt="" margin="10" hspace="10" vspace="5">
            </a>

            <a href="http://www.boyangli.org/">
                <font color=#5672f9><u>Prof. Boyang Li</u></font>
            </a> is a Nanyang Associate Professor at the School of Computer Science and Technology, Nanyang
            Technological University. His research interests lie in computational narrative intelligence, multimodal
            learning, and machine learning. In 2021, he received the
            National Research Foundation Fellowship, a prestigious research award of 2.5 million Singapore Dollars.
            Prior to that, he worked as a senior research scientist at Baidu Research USA and a research scientist
            at Disney Research Pittsburgh, where he led an independent research group. He received his Ph.D. degree from
            Georgia Institute of Technology in 2015 and his Bachelor degree from Nanyang Technological University in
            2008. He currently serves as a senior
            action editor for ACL Rolling Review and an associate editor for IEEE Transactions on Audio, Speech and
            Language Processing. His work has been reported by international media outlets such as the Guardian, New
            Scientist, US National Public Radio, Engadget,
            TechCrunch, and so on.

            </p>

            <b>Talk Title:</b> Unlocking Multimedia Capabilities of Gigantic Pretrained Language Models
            </br>
            <b>Abstract:</b> A large language model (LLM) can be analogized to an enormous treasure box guarded by a
            lock. It contains extensive knowledge, but applying appropriate knowledge to solve the problem at hand
            requires special techniques. In this talk, I will discuss techniques to unlock the capability of LLMs to
            process both visual and linguistic information. VisualGPT is one of the earliest works that finetunes an LLM
            for a vision-language task. InstructBLIP is an instruction-tuned large vision-language model, which set new
            states of the art on several vision-language tasks.
            In addition, I will talk about how to unlock zero-shot capabilities without end-to-end finetuning.
            Plug-and-Play VQA and Img2LLM achieve excellent results on visual question-answering by simply connecting
            existing pretrained networks using natural language and model interpretations. Finally, I will describe a
            new multimodal dataset, Synopses of Movie Narratives, or SyMoN, for story understanding. I will argue that
            story understanding is an important objective in the pursuit of artificial general intelligence (AGI).
            Compared to other multimodal story datasets, the special advantages of SyMoN include (1) event descriptions
            at the right level of granularity, (2) abundant mental state descriptions, (3) the use of diverse
            storytelling techniques, and (4) the provision of easy-to-use automatic performance evaluation.


            <br></br>

            <h3><b>Keynote 3</b></h3>

            <img src="https://liuziwei7.github.io/homepage_files/me.png" class="media-left pull-left" width="170" alt=""
                margin="10" hspace="10" vspace="5">
            </a>

            <a href="https://liuziwei7.github.io/">
                <font color=#5672f9><u>Prof. Ziwei Liu</u></font>
            </a> is currently a Nanyang Assistant Professor at Nanyang Technological University, Singapore. His research
            revolves around computer vision, machine learning and computer graphics. He has published extensively on
            top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurIPS, ICLR, SIGGRAPH,
            TPAMI, TOG and Nature - Machine Intelligence, with around 30,000 citations. He is the recipient of Microsoft
            Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award, CVPR Best
            Paper Award Candidate, WAIC Yunfan Award and ICBS Frontiers of Science Award. He has won the championship in
            major computer vision competitions, including DAVIS Video Segmentation Challenge 2017, MSCOCO Instance
            Segmentation Challenge 2018, FAIR Self-Supervision Challenge 2019, Video Virtual Try-on Challenge 2020 and
            Computer Vision in the Wild Challenge 2022. He is also the lead contributor of several renowned computer
            vision benchmarks and softwares, including CelebA, DeepFashion, MMHuman3D and MMFashion. He serves as an
            Area Chair of CVPR, ICCV, NeurIPS and ICLR, as well as an Associate Editor of IJCV.

            </p>

            <b>Talk Title:</b> Multi-Modal Generative AI with Foundation Models
            </br>
            <b>Abstract:</b> Generating photorealistic and controllable visual contents has been a long-pursuing goal of
            artificial intelligence (AI), with extensive real-world applications. It is also at the core of embodied
            intelligence. In this talk, I will discuss our work in AI-driven visual context generation of humans,
            objects and scenes, with an emphasis on combing the power of neural rendering with large multimodal
            foundation models. Our generative AI framework has shown its effectiveness and generalizability on a wide
            range of tasks.

            <br></br>

            <li>On-site venue: Auditorium A</li>
            <li>Date & time: 2nd November 2023, Local Ottawa Time</li>
            <li>Zoom link: <a href="https://ntu-sg.zoom.us/j/89276840596?pwd=eFRmSWo0TDZSQ2ExTVl0NlVFSm5Kdz09">
                    <font color=#5672f9><u>https://ntu-sg.zoom.us/j/89276840596?pwd=eFRmSWo0TDZSQ2ExTVl0NlVFSm5Kdz09</u>
                    </font>
                </a>
                (Meeting ID: 892 7684 0596, Passcode: 080304) </li>
            <br />
            <style type="text/css">
                #myta td {
                    padding-right: 8px;
                }
            </style>

                <table border="1" id="myta">
                    <tr>
                        <td><strong>Time</strong></td>
                        <td> <strong>Title</strong> </td>
                    </tr>
                    <tr>
                        <td>09:00 AM - 09:10 AM</td>
                        <td> Welcome Message from the Chairs </td>
                    </tr>
                    <tr>
                        <td>09:10 AM - 09:40 AM</td>
                        <td> Keynote 1: Large Generative Models Meet Multimodal Video Intelligence </td>
                    </tr>
                    <tr>
                        <td>09:40 AM - 10:10 AM</td>
                        <td> Keynote 2: Unlocking Multimedia Capabilities of Gigantic Pretrained Language Models </td>
                    </tr>
                    <tr>
                        <td>10:10 AM - 10:40 AM</td>
                        <td> Keynote 3: Multi-Modal Generative AI with Foundation Models </td>
                    </tr>
                    <tr>
                        <td>10:40 AM - 11:10 AM</td>
                        <td> Coffee Break </td>
                    </tr>
                    <tr>
                        <td>11:10 AM - 11:30 AM</td>
                        <td> Presentation 1: SAT: Self-Attention Control for Diffusion Models Training</td>
                    </tr>
                    <tr>
                        <td>11:30 AM - 11:50 AM</td>
                        <td> Presentation 2: NeurSEG: A Segment Driven Deep Neural Model for Nested Named Entity
                            Recognition </td>
                    </tr>
                    <tr>
                        <td>11:50 AM - 14:00 PM</td>
                        <td> Lunch Break </td>
                    </tr>
                    <tr>
                        <td>14:00 PM - 14:20 PM</td>
                        <td> Presentation 3: ImEW: A Framework for Editing Image in the Wild</td>
                    </tr>
                    <tr>
                        <td>14:20 PM - 14:40 PM</td>
                        <td> Presentation 4: Generating Multimodal Augmentations with LLMs from Song Metadata for Music
                            Information Retrieval</td>
                    </tr>
                    <tr>
                        <td>14:40 PM - 15:00 PM</td>
                        <td> Presentation 5: CGSMP: Controllable Generative Summarization via Multimodal Prompt</td>
                    </tr>
                    <tr>
                        <td>15:00 PM - 15:30 PM</td>
                        <td> Coffee Break </td>
                    </tr>
                    <tr>
                        <td>15:30 PM - 15:50 PM</td>
                        <td> Presentation 6: Fashion-GPT: Integrating LLMs with Fashion Retrieval System</td>
                    </tr>
                    <tr>
                        <td>15:50 PM - 16:10 PM</td>
                        <td> Presentation 7: Subsampling of Frequent Words in Text for Pre-training a Vision-Language
                            Model</td>
                    </tr>
                    <tr>
                        <td>16:10 PM - 16:30 PM</td>
                        <td> Presentation 8: Multimodal Data Augmentation for Image Captioning using Diffusion Models
                        </td>
                    </tr>
                    <tr>
                        <td>16:30 PM - 16:40 PM</td>
                        <td> Workshop Closing </td>
                    </tr>
                </table> 
        
        </div>
    </section>-->

    <section id="call" class="">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Call for Papers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                        In recent years, the field of data mining and machine learning has experienced a surge in the
                        scale and complexity of models.
                        These large-scale models, often referred to as "big models", hold immense potential to
                        revolutionize the landscape of information seeking.
                        Moreover, within the realm of language models, recent breakthroughs have given rise to
                        ChatGPT-like powerful models that excel in both natural
                        language understanding and generation. Trained on vast volumes of data, these models offer
                        exciting opportunities for enhancing information seeking experiences.
                        The workshop aims to foster collaboration by bringing together researchers, practitioners, and
                        industry experts from the information seeking and big model communities.
                        Participants will delve into novel techniques, methodologies, and applications related to
                        leveraging big models for information retrieval, recommendation systems, and question-answering.
                    </p>

                    <div class="col-lg-12 text-center">
                        <h3 class="section-subheading">Topics of Interest</h3>
                    </div>

                    <p class="large text-muted">
                        The workshop centers around pioneering research at the intersection of information seeking, big
                        models, and novel technologies.
                        It specifically focuses on five key aspects: (1) revolutionizing information retrieval, e.g.,
                        exploring how big models can enhance efficiency and effectiveness in accessing information;
                        (2) advancing recommender algorithms, e.g., utilizing big models to improve personalized
                        recommendations;
                        (3) enhancing question-answering systems, e.g., leveraging big models for accurate and
                        contextually relevant responses;
                        (4) ensuring trustworthiness, e.g., emphasizing ethical and legal standards when using big
                        models;
                        (5) comprehensive evaluation, e.g., designing robust evaluation methodologies to assess big
                        models' impact. We welcome
                        original submissions across a wide range of topics, including but not limited to:
                    </p>

                    <ul class="large text-muted">
                        <li><strong>Information retrieval with large generative models</strong>
                            <ul>
                                <li>examining how large generative models influence information retrieval and ranking,
                                    and
                                    vice versa.</li>
                            </ul>
                        </li>
                        <li><strong>Query expansion and reformulation strategies</strong>
                            <ul>
                                <li>developing innovative approaches for query expansion and reformulation using
                                    contextual
                                    information from big models.</li>
                            </ul>
                        </li>
                        <li><strong>User studies and experiences with big models</strong>
                            <ul>
                                <li>exploring the effectiveness of big models in addressing diverse information needs
                                    from
                                    a user-centric perspective.</li>
                            </ul>
                        </li>
                        <li><strong>Ethical considerations and responsible AI</strong>
                            <ul>
                                <li>examining ethical implications and propose strategies for responsible AI in the
                                    development and deployment of large generative models.</li>
                            </ul>
                        </li>
                        <li><strong>Architectural integration into search engines</strong>
                            <ul>
                                <li>discussing architectural considerations for integrating big models into existing
                                    search
                                    products.</li>
                            </ul>
                        </li>
                        <li><strong>Scalability challenges in deploying big models</strong>
                            <ul>
                                <li>exploring solutions ensuring efficient and reliable performance at scale.</li>
                            </ul>
                        </li>
                        <li><strong>Applications in specialized domains</strong>
                            <ul>
                                <li>investigating the role of big models in domains such as healthcare, education,
                                    finance, legal, and others.</li>
                            </ul>
                        </li>
                        <li><strong>Robust evaluation metrics</strong>
                            <ul>
                                <li>exploring reliable metrics for assessing big models' performance in information
                                    seeking.</li>
                            </ul>
                        </li>
                        <li><strong>Multimodal information retrieval</strong>
                            <ul>
                                <li>investigating theoretical, algorithmic or practical solutions addressing problems
                                    across the domain of multimedia and information retrieval with big models.</li>
                            </ul>
                        </li>
                        <li><strong>Integration in recommendation systems</strong>
                            <ul>
                                <li>exploring how big models enhance recommendation systems for personalized content
                                    discovery and accurate recommendations.</li>
                            </ul>
                        </li>
                        <li><strong>Advancements in question-answering</strong>
                            <ul>
                                <li>discussing advancements in using big models for both open-domain and domain-specific
                                    question-answering.</li>
                            </ul>
                        </li>
                    </ul>

                    <div class="col-lg-12 text-center">
                        <h3 class="section-subheading">Submission Guidelines</h3>
                    </div>
                    <p class="large text-muted">
                        Prospective authors are invited to submit original research papers that address the topics of
                        interest for the workshop.
                        Paper submissions should be limited to a maximum of ten (10) pages, in the <a
                            href="https://www.ieee.org/conferences/publishing/templates.html">IEEE 2-column format</a>,
                        including the bibliography and any possible appendices. Submissions longer than 10 pages will be
                        rejected without review.
                        All submissions will be double-blind reviewed by the Program Committee on the basis of technical
                        quality, relevance to scope of the workshop, originality, significance, and clarity.
                        Please refer to the ICDM regular <a href="https://icdm2024.org/call_for_papers/">Submission
                            Guidelines</a> for more information.
                    </p>

                    <!-- <p class="large text-muted">
                        Submission site: <a href="https://easychair.org/my/conference?conf=BigIS24">https://easychair.org/my/conference?conf=BigIS24</a>.
                    </p> -->
                    <div class="col-lg-12 text-center">
                        <h3 class="section-subheading">Important Dates</h3>
                    </div>
                    <p class="large text-muted">
                        <strong>Workshop papers submission:</strong> September 17, 2024 <br> <!--<del></del>-->
                        <strong>Notification of workshop papers acceptance to authors:</strong> October 7, 2024 <br>
                        <!--<del></del>-->
                        <strong>Camera-ready deadline and copyright form:</strong> October 11, 2024 <br>
                        <strong>Workshops date:</strong> December 9, 2024 <br>

                    </p>
                    <p class="large text-muted">
                        <strong>Paper submission link:</strong> <a
                            href="https://wi-lab.com/cyberchair/2024/icdm24/scripts/submit.php?subarea=S09&amp;undisplay_detail=1&amp;wh=/cyberchair/2024/icdm24/scripts/ws_submit.php">Workshop
                            on Information Seeking with Big Models</a><br>
                        The submission deadline is at 11:59 p.m. of the stated deadline date&nbsp;<a
                            href="https://time.is/Anywhere_on_Earth">Anywhere on Earth</a>.
                    </p>


                    <p></p>

                </div>
            </div>
        </div>
    </section>

    <section id="organizer" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
                    <h2 class="section-heading">Workshop Organizers </h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>

            <div class="row">


                <div class="col-sm-1">
                     
                </div>

                <!--<div class="col-sm-1">
                    &thinsp;
                </div>-->

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/zhengwang.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://zhengwang125.github.io/">Zheng Wang</a></h4>
                        <p class="text-muted">Researcher</p>
                        <p class="text-muted">Huawei Singapore Research Center</p>
                    </div>
                </div>

                <div class="col-sm-1">
                     
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/shuxianteo.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://bigis24.github.io/">Shu Xian Teo</a></h4>
                        <p class="text-muted">Researcher</p>
                        <p class="text-muted">Huawei Singapore Research Center</p>
                    </div>
                </div>


                <div class="col-sm-1">
                     
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/weishi.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://bigis24.github.io/bigis24.github.io">Wei Shi</a></h4>
                        <p class="text-muted">Researcher</p>
                        <p class="text-muted">Huawei Singapore Research Center</p>
                    </div>
                </div>

                <div class="col-sm-1">
                     
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/kuntan.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://sora-city.github.io/">Kun Tan</a></h4>
                        <p class="text-muted">Researcher</p>
                        <p class="text-muted">Huawei Technologies, Co., Ltd.</p>
                    </div>
                </div>


                <div class="col-sm-1">
                     
                </div>

            </div>

            <div class="row">

                <div class="col-sm-1">
                     
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/xiangnanhe.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://hexiangnan.github.io/">Xiangnan He</a></h4>
                        <p class="text-muted">Professor</p>
                        <p class="text-muted">University of Science and Technology of China</p>
                    </div>
                </div>

                <div class="col-sm-1">
                     
                </div>


                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/chenglong.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://personal.ntu.edu.sg/c.long/index.html">Cheng Long</a></h4>
                        <p class="text-muted">Professor</p>
                        <p class="text-muted">Nanyang Technological University</p>
                    </div>
                </div>
                <div class="col-sm-1">
                     
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./BigIS Workshop_files/gaocong.jpg"
                            class="img-responsive img-circle" alt="" width="200" height="200">
                        <h4><a href="https://personal.ntu.edu.sg/gaocong/">Gao Cong</a></h4>
                        <p class="text-muted">Professor</p>
                        <p class="text-muted">Nanyang Technological University</p>
                    </div>
                </div>

            </div>

        </div>
    </section>


    <!--==========================================
    =            Speakers Section            =
    ===========================================-->

    <section id="speakers" class="">

        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Speakers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>

            <h3><b>Keynote 1</b></h3>

            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->

            <img src="https://personal.ntu.edu.sg/axsun/images/AixinWeb.jpg" class="media-left pull-left" width="170"
                alt="" margin="10" hspace="10" vspace="5">
            </a>

            <a href="https://personal.ntu.edu.sg/axsun/">
                <font color=#5672f9><u>Prof. Aixin Sun</u></font>
            </a> is an Associate Professor at the College of Computing and Data Science (CCDS), Nanyang Technological
            University (NTU),
            Singapore. He received B.A.Sc (1st class honours) and Ph.D. both in Computer Engineering from NTU in 2001
            and 2004 respectively.
            His current research interests include information retrieval, text mining, recommender systems, and social
            computing.
            Dr. Sun is an associate editor of ACM Transactions on Information Systems (TOIS), ACM Transactions on
            Intelligent Systems and Technology (TIST),
            ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), Neurocomputing, and
            editorial board member of Journal of
            the Association for Information Science and Technology (JASIST). He has also served as Area Chair,
            Senior PC member for many conferences including SIGIR, WWW, WSDM, and NeurIPS.
            </p>

            <b>Talk Title:</b> Question-Answering via Large Vision-Language Models
            </br>
            <b>Abstract:</b> For a long time, the development of natural language processing (NLP) has been based on the
            assumption that data is processed in raw text format.
            However, the documents we encounter in daily life often contain rich structures, including nicely formatted
            pages with visual elements like figures, charts, photos,
            and tables. The rapid advancements in large vision-language models (LVLMs) provide an opportunity to
            reconsider traditional NLP pipelines. With LVLMs,
            we can read PDF/Word files directly from screenshots (e.g., PNG files), and this simplified process
            eliminates the need for parsing documents before LLMs can answer
            questions based on extracted (or OCR-ed) text. More importantly, this end-to-end approach preserves both
            textual and visual information in its entirety.
            In this talk, I will introduce MMLongBench-Doc, an evaluation of 14 LVLMs and 10 LLMs on question-answering
            tasks over long documents with rich visual elements.
            I will also discuss key insights gained from this benchmarking and their implications for the future of NLP.

            <br></br>

            <h3><b>Keynote 2</b></h3>

            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->

            <img src="https://www.a-star.edu.sg/images/56centreforfrontierairesearchcfarlibraries/default-album/team--feng-shanshan.png?sfvrsn=b9f7abe4_0"
                class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5">
            </a>

            <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/feng-shanshan">
                <font color=#5672f9><u>Dr. Shanshan Feng</u></font>
            </a> is currently a senior scientist at the Centre for Frontier AI Research, Institute of High Performance
            Computing, Agency for Science,
            Technology and Research (A*STAR), Singapore. He received the Ph.D. degree from Nanyang Technological
            University, Singapore in 2017, and his B.E. degree
            from the University of Science and Technology of China in 2011. His current research interests include big
            data analytics, social graph learning,
            and recommender systems. He has published more than 60 papers in prestigious journals and conferences
            including IEEE TPAMI, IEEE TKDE, IEEE TNNLS, IEEE TGRS,
            SIGKDD, SIGIR, ICDE, VLDB, etc. He served as a PC member of SIGIR, SIGKDD, ICDE, ICDM, AAAI, and IJCAI, etc.

            </p>

            <b>Talk Title:</b> Multi-Objective Optimization in Generative Recommendations
            </br>
            <b>Abstract:</b> Recent advancements in recommendation systems have primarily focused on improving
            single-objective metrics,
            such as accuracy. However, the evolving landscape of artificial general intelligence and the rise of large
            language models (LLMs)
            have opened new avenues for handling multiple tasks and objectives simultaneously within recommendation
            systems. These versatile
            foundational models offer enhanced flexibility in addressing multi-task and multi-objective challenges,
            making it increasingly critical
            to explore and understand this emerging field. In this talk, I will present a comprehensive review of
            multi-objective optimization techniques
            in recommendation systems, focusing on the integration of generative AI methods. The discussion will cover
            key tasks, methods, applications,
            and recent developments related to Multi-Objective Generative Recommendations. The goal is to bridge the gap
            in existing literature by highlighting
            the importance of multi-objective approaches in the next generation of recommendation systems.

            <br></br>

            <h3><b>Keynote 3</b></h3>

            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->

            <img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=Rt8ppJsAAAAJ&citpid=2"
                class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5">
            </a>

            <a href="https://sites.google.com/view/hossein-esfandiari/home">
                <font color=#5672f9><u>Dr. Hossein Esfandiari</u></font>
            </a>
            is a Senior Research Scientist at the Google NYC Algorithms and Optimization Team. Previously, he was a
            Postdoctoral Researcher at Harvard University in the Theory of Computation group, where he was advised by
            Professor Michael Mitzenmacher. He received his Ph.D. in Computer Science from the University of Maryland,
            under the guidance of Professor Mohammad T. HajiAghayi. He completed his undergraduate studies in Computer
            Engineering at Sharif University of Technology.
            </p>

            <b>Talk Title:</b> Providing Large-Scale Privacy for Complex Models via Smooth Anonymity
            </br>
            <b>Abstract:</b> When working with user data, providing well-defined privacy guarantees is paramount.
            Here, we aim to manipulate and share a complex learning model or an entire dataset with a third party
            privately.
            As our first main result, we prove that any differentially private mechanism that maintains a reasonable
            similarity
            with the initial dataset is doomed to have a very weak privacy guarantee in the worst case. In such
            situations, we
            need to look into other privacy notions such as k-anonymity. Hence, we consider a variation of k-anonymity,
            which we
            call smooth-k-anonymity, and design a very large-scale algorithm that efficiently provides
            smooth-k-anonymity for billions of entries.
            Our empirical evaluations show that our algorithm improves the performance in downstream machine learning
            tasks on anonymized data.

            <br></br>
    </section>

    <!--===========================================
      =            Workshop Arrangement            =
      ============================================-->

    <section id="schedule" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Workshop Schedule</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <p align="center">
                <!-- <img src="./res/schedule.png" width="90%"> -->
                <!-- <big><b>TBD</b></big> -->
                <!-- <font color="Blue"><i><strong>TBA </strong></i></font> -->
                <li>On-site venue: Room 6</li>
                <li>Date & Time: December 9th Morning (Abu Dhabi Local Time)</li>
                <li>Zoom link: <a href="https://ntu-sg.zoom.us/j/82551541095?pwd=WJmfGaknmEZhoHWjL88ULYTxEnhoVk.1">
                        <font color=#5672f9><u>Meeting ID: 825 5154 1095, Passcode: 241209</u>
                        </font>
                    </a></li>
                <br />
                <style type="text/css">
                    #myta td {
                        padding-right: 8px;
                    }
                </style>
            <table border="1" id="myta">

                <tr>
                    <td><strong>Time </strong></td>
                    <td> <strong>Title</strong> </td>
                </tr>
                <tr>
                    <td>11:30 - 11:40</td>
                    <td> Welcome Message from the Chairs </td>
                </tr>
                <tr>
                    <td>11:40 - 12:15</td>
                    <td> Keynote 1: Question-Answering via Large Vision-Language Models </td>
                </tr>
                <tr>
                    <td>12:15 - 12:50</td>
                    <td> Keynote 2: Multi-Objective Optimization in Generative Recommendations </td>
                </tr>
                <tr>
                    <td>12:50 - 13:25</td>
                    <td> Keynote 3: Providing Large-Scale Privacy for Complex Models via Smooth Anonymity </td>
                </tr>
                <tr>
                    <td>13:25 - 13:35</td>
                    <td> Presentation 1: SRSA: A Cost-Efficient Strategy-Router Search Agent for Contextual Queries
                    </td>
                </tr>
                <tr>
                    <td>13:35 - 13:45</td>
                    <td> Presentation 2: Emotion Retrieval Using Generative Models: An Exploratory Study </td>
                </tr>
                <tr>
                    <td>13:45 - 13:55</td>
                    <td> Presentation 3: Foundation Models for Course Equivalency Evaluation </td>
                </tr>
                <tr>
                    <td>13:55 - 14:00</td>
                    <td> Workshop Closing </td>
                </tr>
            </table>
            <br>
            <br>
    </section>

    <section id="contact" class="">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Contact</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                        In case of questions, contact us via an email to <a href="mailto:zheng011@e.ntu.edu.sg">Zheng
                            Wang</a>.
                    </p>
                </div>
            </div>
        </div>
    </section>


    <!-- jQuery -->
    <script src="./BigIS Workshop_files/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./BigIS Workshop_files/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="./BigIS Workshop_files/jquery.easing.min.js"></script>
    <script src="./BigIS Workshop_files/classie.js"></script>
    <script src="./BigIS Workshop_files/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="./BigIS Workshop_files/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="./BigIS Workshop_files/agency.js"></script>


</body>

</html>
